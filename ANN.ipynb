{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation of ANN from scratch using only Numpy..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only thing we need is Numpy...\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a Random seed...\n",
    "# np.andom.seed() provides an essential input that enables NumPy\n",
    "# to generate pseudo-random numbers for random processes.\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sigmoidal Activation Function...\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp( -1 * x))\n",
    "\n",
    "# To simplify this, we will represent that differential equation as d_sigmoid. \n",
    "\n",
    "def d_sigmoid(x):\n",
    "    return sigmoid(x) * (1- sigmoid(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some sample data for our ANN.\n",
    "# We take input value 'x' and\n",
    "# target value 'y'\n",
    "\n",
    "x = np.array([\n",
    "    [0,0,1],\n",
    "    [0,1,1],\n",
    "    [1,0,1],\n",
    "    [1,1,1]\n",
    "])\n",
    "\n",
    "y = np.array([\n",
    "    [0],\n",
    "    [0],\n",
    "    [0],\n",
    "    [1]\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Declare Hyper Parameters...\n",
    "\n",
    "# Initialize weight(s)...\n",
    "w1 = np.random.randn(3,1)\n",
    "\n",
    "# Set the number of epoch\n",
    "numer_of_epoch = 100\n",
    "\n",
    "# Set the learning rate\n",
    "learning_rate = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Epoch :  0  current loss : 0.11293240452690462\n",
      "Current Epoch :  1  current loss : 0.11061045469556602\n",
      "Current Epoch :  2  current loss : 0.10841998634013196\n",
      "Current Epoch :  3  current loss : 0.10634746444246652\n",
      "Current Epoch :  4  current loss : 0.10438119645547413\n",
      "Current Epoch :  5  current loss : 0.10251107800511905\n",
      "Current Epoch :  6  current loss : 0.1007283585804454\n",
      "Current Epoch :  7  current loss : 0.09902543244860601\n",
      "Current Epoch :  8  current loss : 0.09739565671979347\n",
      "Current Epoch :  9  current loss : 0.09583319624216735\n",
      "Current Epoch :  10  current loss : 0.09433289358769031\n",
      "Current Epoch :  11  current loss : 0.09289016158456455\n",
      "Current Epoch :  12  current loss : 0.09150089548643037\n",
      "Current Epoch :  13  current loss : 0.0901614018051529\n",
      "Current Epoch :  14  current loss : 0.08886834096772975\n",
      "Current Epoch :  15  current loss : 0.08761868121025616\n",
      "Current Epoch :  16  current loss : 0.08640966143605569\n",
      "Current Epoch :  17  current loss : 0.08523876110085096\n",
      "Current Epoch :  18  current loss : 0.08410367551798631\n",
      "Current Epoch :  19  current loss : 0.08300229528382344\n",
      "Current Epoch :  20  current loss : 0.08193268879750171\n",
      "Current Epoch :  21  current loss : 0.08089308708565253\n",
      "Current Epoch :  22  current loss : 0.07988187034055363\n",
      "Current Epoch :  23  current loss : 0.07889755574132098\n",
      "Current Epoch :  24  current loss : 0.07793878625535745\n",
      "Current Epoch :  25  current loss : 0.0770043202155313\n",
      "Current Epoch :  26  current loss : 0.07609302154188251\n",
      "Current Epoch :  27  current loss : 0.07520385052941102\n",
      "Current Epoch :  28  current loss : 0.07433585515975528\n",
      "Current Epoch :  29  current loss : 0.0734881629179503\n",
      "Current Epoch :  30  current loss : 0.07265997310908444\n",
      "Current Epoch :  31  current loss : 0.07185054967618287\n",
      "Current Epoch :  32  current loss : 0.07105921452216504\n",
      "Current Epoch :  33  current loss : 0.07028534133695795\n",
      "Current Epoch :  34  current loss : 0.06952834992711494\n",
      "Current Epoch :  35  current loss : 0.06878770104057583\n",
      "Current Epoch :  36  current loss : 0.06806289167423393\n",
      "Current Epoch :  37  current loss : 0.06735345084723937\n",
      "Current Epoch :  38  current loss : 0.06665893581879392\n",
      "Current Epoch :  39  current loss : 0.06597892872576477\n",
      "Current Epoch :  40  current loss : 0.06531303361284468\n",
      "Current Epoch :  41  current loss : 0.06466087382622841\n",
      "Current Epoch :  42  current loss : 0.06402208974081089\n",
      "Current Epoch :  43  current loss : 0.06339633679066917\n",
      "Current Epoch :  44  current loss : 0.0627832837729662\n",
      "Current Epoch :  45  current loss : 0.06218261139630815\n",
      "Current Epoch :  46  current loss : 0.06159401104588688\n",
      "Current Epoch :  47  current loss : 0.06101718373934671\n",
      "Current Epoch :  48  current loss : 0.060451839249133\n",
      "Current Epoch :  49  current loss : 0.059897695369029384\n",
      "Current Epoch :  50  current loss : 0.0593544773045966\n",
      "Current Epoch :  51  current loss : 0.05882191716923406\n",
      "Current Epoch :  52  current loss : 0.05829975356954456\n",
      "Current Epoch :  53  current loss : 0.057787731265562275\n",
      "Current Epoch :  54  current loss : 0.0572856008931718\n",
      "Current Epoch :  55  current loss : 0.056793118737691856\n",
      "Current Epoch :  56  current loss : 0.05631004654910381\n",
      "Current Epoch :  57  current loss : 0.05583615139077286\n",
      "Current Epoch :  58  current loss : 0.05537120551473733\n",
      "Current Epoch :  59  current loss : 0.05491498625773225\n",
      "Current Epoch :  60  current loss : 0.054467275953075295\n",
      "Current Epoch :  61  current loss : 0.05402786185438257\n",
      "Current Epoch :  62  current loss : 0.053596536067811285\n",
      "Current Epoch :  63  current loss : 0.05317309549015117\n",
      "Current Epoch :  64  current loss : 0.05275734175062303\n",
      "Current Epoch :  65  current loss : 0.052349081154694437\n",
      "Current Epoch :  66  current loss : 0.05194812462860471\n",
      "Current Epoch :  67  current loss : 0.051554287663608436\n",
      "Current Epoch :  68  current loss : 0.051167390259210514\n",
      "Current Epoch :  69  current loss : 0.050787256864880795\n",
      "Current Epoch :  70  current loss : 0.05041371631991243\n",
      "Current Epoch :  71  current loss : 0.05004660179122852\n",
      "Current Epoch :  72  current loss : 0.04968575070905362\n",
      "Current Epoch :  73  current loss : 0.049331004700454484\n",
      "Current Epoch :  74  current loss : 0.04898220952082108\n",
      "Current Epoch :  75  current loss : 0.04863921498341016\n",
      "Current Epoch :  76  current loss : 0.04830187488710924\n",
      "Current Epoch :  77  current loss : 0.04797004694260537\n",
      "Current Epoch :  78  current loss : 0.04764359269715833\n",
      "Current Epoch :  79  current loss : 0.04732237745818747\n",
      "Current Epoch :  80  current loss : 0.047006270215884854\n",
      "Current Epoch :  81  current loss : 0.04669514356506533\n",
      "Current Epoch :  82  current loss : 0.046388873626460346\n",
      "Current Epoch :  83  current loss : 0.04608733996765446\n",
      "Current Epoch :  84  current loss : 0.045790425523854844\n",
      "Current Epoch :  85  current loss : 0.045498016518672676\n",
      "Current Epoch :  86  current loss : 0.04521000238508549\n",
      "Current Epoch :  87  current loss : 0.04492627568673588\n",
      "Current Epoch :  88  current loss : 0.04464673203971139\n",
      "Current Epoch :  89  current loss : 0.04437127003493757\n",
      "Current Epoch :  90  current loss : 0.044099791161304636\n",
      "Current Epoch :  91  current loss : 0.043832199729636284\n",
      "Current Epoch :  92  current loss : 0.04356840279759876\n",
      "Current Epoch :  93  current loss : 0.04330831009563675\n",
      "Current Epoch :  94  current loss : 0.04305183395401331\n",
      "Current Epoch :  95  current loss : 0.04279888923102131\n",
      "Current Epoch :  96  current loss : 0.04254939324242448\n",
      "Current Epoch :  97  current loss : 0.04230326569217839\n",
      "Current Epoch :  98  current loss : 0.04206042860447348\n",
      "Current Epoch :  99  current loss : 0.04182080625713519\n"
     ]
    }
   ],
   "source": [
    "# Here is the set of training opertions...\n",
    "for iter in range(numer_of_epoch):\n",
    "\n",
    "    # 1. Make the Dot Product operation\n",
    "    layer_1 = x.dot(w1)\n",
    "    layer_1_act = sigmoid(layer_1)\n",
    "\n",
    "    # loss function - (Mean Square Error)MSE 0.5\n",
    "    loss = np.square(layer_1_act - y) / (len(layer_1_act)  * 2)\n",
    "\n",
    "    print (\"Current Epoch : \",iter ,\" current loss :\", loss.sum())\n",
    "\n",
    "    # Stocastic Gradient Descent(SGD) - BATCH\n",
    "    # this is the training algorithm here...\n",
    "    grad_1_part_1 = (layer_1_act - y) / len(layer_1_act)\n",
    "    grad_1_part_2 = d_sigmoid(layer_1)\n",
    "    grad_1_part_3 = x\n",
    "    grad_1 = grad_1_part_3.T.dot(grad_1_part_1 * grad_1_part_2)\n",
    "\n",
    "    # Weight Update\n",
    "    w1 -=  learning_rate * grad_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we declare a single layer NN...\n",
    "\n",
    "layer_1 = x.dot(w1)\n",
    "layer_1_act = sigmoid(layer_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Final :  [0.09554338 0.24983353 0.3376184  0.61640766]\n",
      "Final Round:  [0. 0. 0. 1.]\n",
      "Ground Truth :  [0 0 0 1]\n",
      "W1 :  [ 1.57382846  1.1482537  -2.24775401]\n"
     ]
    }
   ],
   "source": [
    "# what we get?\n",
    "print (\"\\n\\nFinal : \" ,layer_1_act[:,-1])\n",
    "print (\"Final Round: \" ,np.round(layer_1_act[:,-1]))\n",
    "print (\"Ground Truth : \",y[:,-1])\n",
    "print (\"W1 : \",w1[:,-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------------That's it for this cycle-----------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now what if we update the weights while training?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Declare Hyper Parameters...\n",
    "\n",
    "# Initialize weight(s)...\n",
    "w1 = np.random.randn(3,1)\n",
    "\n",
    "# Set the number of epoches\n",
    "numer_of_epoch = 100\n",
    "\n",
    "# Set the learning rate\n",
    "learning_rate = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Current Epoch :  0  Current Accuracy :  0.8873114091385832  current loss : 0.11268859086141686  Current Learning Rate:  10\n",
      "\n",
      "Current Epoch :  1  Current Accuracy :  0.891453031946667  current loss : 0.10854696805333297  Current Learning Rate:  10\n",
      "\n",
      "Current Epoch :  2  Current Accuracy :  0.8981444126739027  current loss : 0.10185558732609733  Current Learning Rate:  10\n",
      "\n",
      "Current Epoch :  3  Current Accuracy :  0.909008927315297  current loss : 0.09099107268470297  Current Learning Rate:  10\n",
      "\n",
      "Current Epoch :  4  Current Accuracy :  0.9241317597903226  current loss : 0.07586824020967738  Current Learning Rate:  10\n",
      "\n",
      "Current Epoch :  5  Current Accuracy :  0.937033494263932  current loss : 0.06296650573606796  Current Learning Rate:  10\n",
      "\n",
      "Current Epoch :  6  Current Accuracy :  0.943871370195538  current loss : 0.05612862980446194  Current Learning Rate:  10\n",
      "\n",
      "Current Epoch :  7  Current Accuracy :  0.9487263982900559  current loss : 0.051273601709944105  Current Learning Rate:  10\n",
      "\n",
      "Current Epoch :  8  Current Accuracy :  0.9526781409616775  current loss : 0.047321859038322533  Current Learning Rate:  10\n",
      "\n",
      "Current Epoch :  9  Current Accuracy :  0.9559445522672629  current loss : 0.04405544773273712  Current Learning Rate:  10\n",
      "\n",
      "Current Epoch :  10  Current Accuracy :  0.9586901718228329  current loss : 0.04130982817716709  Current Learning Rate:  10\n",
      "\n",
      "Current Epoch :  11  Current Accuracy :  0.9610389865367908  current loss : 0.038961013463209196  Current Learning Rate:  10\n",
      "\n",
      "Current Epoch :  12  Current Accuracy :  0.9630822095679323  current loss : 0.036917790432067624  Current Learning Rate:  10\n",
      "\n",
      "Current Epoch :  13  Current Accuracy :  0.9648862975367558  current loss : 0.035113702463244224  Current Learning Rate:  10\n",
      "\n",
      "Current Epoch :  14  Current Accuracy :  0.966499696789933  current loss : 0.033500303210067056  Current Learning Rate:  10\n",
      "\n",
      "Current Epoch :  15  Current Accuracy :  0.9679580027574508  current loss : 0.032041997242549226  Current Learning Rate:  10\n",
      "\n",
      "Current Epoch :  16  Current Accuracy :  0.9692877097324722  current loss : 0.030712290267527777  Current Learning Rate:  10\n",
      "\n",
      "Current Epoch :  17  Current Accuracy :  0.9705088624774787  current loss : 0.029491137522521325  Current Learning Rate:  10\n",
      "\n",
      "Current Epoch :  18  Current Accuracy :  0.9716369062136677  current loss : 0.028363093786332276  Current Learning Rate:  10\n",
      "\n",
      "Current Epoch :  19  Current Accuracy :  0.9726839715005229  current loss : 0.02731602849947716  Current Learning Rate:  10\n",
      "\n",
      "Current Epoch :  20  Current Accuracy :  0.9736597679613719  current loss : 0.026340232038628078  Current Learning Rate:  10\n",
      "\n",
      "Current Epoch :  21  Current Accuracy :  0.9745722095126375  current loss : 0.02542779048736249  Current Learning Rate:  10\n",
      "\n",
      "Current Epoch :  22  Current Accuracy :  0.9754278556451773  current loss : 0.024572144354822693  Current Learning Rate:  10\n",
      "\n",
      "Current Epoch :  23  Current Accuracy :  0.9762322263731246  current loss : 0.02376777362687546  Current Learning Rate:  10\n",
      "\n",
      "Current Epoch :  24  Current Accuracy :  0.9769900299358347  current loss : 0.023009970064165296  Current Learning Rate:  10\n",
      "\n",
      "Current Epoch :  25  Current Accuracy :  0.977705329770255  current loss : 0.02229467022974494  Current Learning Rate:  10\n",
      "\n",
      "Current Epoch :  26  Current Accuracy :  0.9783816688026544  current loss : 0.021618331197345554  Current Learning Rate:  10\n",
      "\n",
      "Current Epoch :  27  Current Accuracy :  0.9790221634127904  current loss : 0.020977836587209687  Current Learning Rate:  10\n",
      "\n",
      "Current Epoch :  28  Current Accuracy :  0.9796295755866915  current loss : 0.02037042441330849  Current Learning Rate:  10\n",
      "\n",
      "Current Epoch :  29  Current Accuracy :  0.9802063691796495  current loss : 0.019793630820350468  Current Learning Rate:  10\n",
      "\n",
      "Current Epoch :  30  Current Accuracy :  0.9807547544467676  current loss : 0.019245245553232367  Current Learning Rate:  10\n",
      "\n",
      "Current Epoch :  31  Current Accuracy :  0.9812767237905705  current loss : 0.018723276209429485  Current Learning Rate:  10\n",
      "\n",
      "Current Epoch :  32  Current Accuracy :  0.9817740808417533  current loss : 0.018225919158246662  Current Learning Rate:  10\n",
      "\n",
      "Current Epoch :  33  Current Accuracy :  0.9822484644090812  current loss : 0.017751535590918837  Current Learning Rate:  10\n",
      "\n",
      "Current Epoch :  34  Current Accuracy :  0.9827013684269416  current loss : 0.017298631573058477  Current Learning Rate:  10\n",
      "\n",
      "Current Epoch :  35  Current Accuracy :  0.9831341587399292  current loss : 0.016865841260070798  Current Learning Rate:  10\n",
      "\n",
      "Current Epoch :  36  Current Accuracy :  0.9835480873565842  current loss : 0.016451912643415785  Current Learning Rate:  10\n",
      "\n",
      "Current Epoch :  37  Current Accuracy :  0.9839443046542371  current loss : 0.016055695345762948  Current Learning Rate:  10\n",
      "\n",
      "Current Epoch :  38  Current Accuracy :  0.9843238699069177  current loss : 0.015676130093082375  Current Learning Rate:  10\n",
      "\n",
      "Current Epoch :  39  Current Accuracy :  0.9846877604268128  current loss : 0.015312239573187184  Current Learning Rate:  10\n",
      "\n",
      "Current Epoch :  40  Current Accuracy :  0.9850368795487373  current loss : 0.014963120451262699  Current Learning Rate:  10\n",
      "\n",
      "Current Epoch :  41  Current Accuracy :  0.9853720636408693  current loss : 0.014627936359130752  Current Learning Rate:  10\n",
      "\n",
      "Current Epoch :  42  Current Accuracy :  0.9856940882896177  current loss : 0.01430591171038233  Current Learning Rate:  10\n",
      "\n",
      "Current Epoch :  43  Current Accuracy :  0.9860036737790998  current loss : 0.01399632622090016  Current Learning Rate:  10\n",
      "\n",
      "Current Epoch :  44  Current Accuracy :  0.9863014899642876  current loss : 0.013698510035712399  Current Learning Rate:  10\n",
      "\n",
      "Current Epoch :  45  Current Accuracy :  0.9865881606199581  current loss : 0.013411839380041913  Current Learning Rate:  10\n",
      "\n",
      "Current Epoch :  46  Current Accuracy :  0.986864267334085  current loss : 0.013135732665914909  Current Learning Rate:  10\n",
      "\n",
      "Current Epoch :  47  Current Accuracy :  0.98713035300344  current loss : 0.012869646996559972  Current Learning Rate:  10\n",
      "\n",
      "Current Epoch :  48  Current Accuracy :  0.9873869249803455  current loss : 0.012613075019654429  Current Learning Rate:  10\n",
      "\n",
      "Current Epoch :  49  Current Accuracy :  0.9876344579123001  current loss : 0.012365542087699952  Current Learning Rate:  10\n",
      "\n",
      "Current Epoch :  50  Current Accuracy :  0.9878733963102296  current loss : 0.012126603689770345  Current Learning Rate:  10\n",
      "\n",
      "Current Epoch :  51  Current Accuracy :  0.9881041568761725  current loss : 0.011895843123827538  Current Learning Rate:  1\n",
      "\n",
      "Current Epoch :  52  Current Accuracy :  0.9881266249435935  current loss : 0.011873375056406524  Current Learning Rate:  1\n",
      "\n",
      "Current Epoch :  53  Current Accuracy :  0.9881490169655349  current loss : 0.011850983034465062  Current Learning Rate:  1\n",
      "\n",
      "Current Epoch :  54  Current Accuracy :  0.9881713333054296  current loss : 0.011828666694570378  Current Learning Rate:  1\n",
      "\n",
      "Current Epoch :  55  Current Accuracy :  0.9881935743244787  current loss : 0.01180642567552127  Current Learning Rate:  1\n",
      "\n",
      "Current Epoch :  56  Current Accuracy :  0.9882157403816687  current loss : 0.01178425961833132  Current Learning Rate:  1\n",
      "\n",
      "Current Epoch :  57  Current Accuracy :  0.9882378318337878  current loss : 0.011762168166212238  Current Learning Rate:  1\n",
      "\n",
      "Current Epoch :  58  Current Accuracy :  0.9882598490354426  current loss : 0.011740150964557368  Current Learning Rate:  1\n",
      "\n",
      "Current Epoch :  59  Current Accuracy :  0.9882817923390746  current loss : 0.011718207660925352  Current Learning Rate:  1\n",
      "\n",
      "Current Epoch :  60  Current Accuracy :  0.9883036620949761  current loss : 0.011696337905023911  Current Learning Rate:  1\n",
      "\n",
      "Current Epoch :  61  Current Accuracy :  0.9883254586513062  current loss : 0.011674541348693793  Current Learning Rate:  1\n",
      "\n",
      "Current Epoch :  62  Current Accuracy :  0.9883471823541071  current loss : 0.011652817645892896  Current Learning Rate:  1\n",
      "\n",
      "Current Epoch :  63  Current Accuracy :  0.9883688335473195  current loss : 0.011631166452680445  Current Learning Rate:  1\n",
      "\n",
      "Current Epoch :  64  Current Accuracy :  0.9883904125727986  current loss : 0.011609587427201402  Current Learning Rate:  1\n",
      "\n",
      "Current Epoch :  65  Current Accuracy :  0.988411919770329  current loss : 0.011588080229670956  Current Learning Rate:  1\n",
      "\n",
      "Current Epoch :  66  Current Accuracy :  0.9884333554776408  current loss : 0.011566644522359178  Current Learning Rate:  1\n",
      "\n",
      "Current Epoch :  67  Current Accuracy :  0.9884547200304242  current loss : 0.011545279969575814  Current Learning Rate:  1\n",
      "\n",
      "Current Epoch :  68  Current Accuracy :  0.9884760137623448  current loss : 0.011523986237655194  Current Learning Rate:  1\n",
      "\n",
      "Current Epoch :  69  Current Accuracy :  0.9884972370050588  current loss : 0.011502762994941292  Current Learning Rate:  1\n",
      "\n",
      "Current Epoch :  70  Current Accuracy :  0.9885183900882271  current loss : 0.01148160991177289  Current Learning Rate:  1\n",
      "\n",
      "Current Epoch :  71  Current Accuracy :  0.9885394733395311  current loss : 0.011460526660468921  Current Learning Rate:  0.1\n",
      "\n",
      "Current Epoch :  72  Current Accuracy :  0.9885415762740354  current loss : 0.011458423725964596  Current Learning Rate:  0.1\n",
      "\n",
      "Current Epoch :  73  Current Accuracy :  0.9885436785150494  current loss : 0.011456321484950616  Current Learning Rate:  0.1\n",
      "\n",
      "Current Epoch :  74  Current Accuracy :  0.9885457800628964  current loss : 0.011454219937103556  Current Learning Rate:  0.1\n",
      "\n",
      "Current Epoch :  75  Current Accuracy :  0.9885478809178999  current loss : 0.011452119082100125  Current Learning Rate:  0.1\n",
      "\n",
      "Current Epoch :  76  Current Accuracy :  0.9885499810803827  current loss : 0.011450018919617274  Current Learning Rate:  0.1\n",
      "\n",
      "Current Epoch :  77  Current Accuracy :  0.9885520805506679  current loss : 0.011447919449332116  Current Learning Rate:  0.1\n",
      "\n",
      "Current Epoch :  78  Current Accuracy :  0.988554179329078  current loss : 0.011445820670921984  Current Learning Rate:  0.1\n",
      "\n",
      "Current Epoch :  79  Current Accuracy :  0.9885562774159355  current loss : 0.011443722584064397  Current Learning Rate:  0.1\n",
      "\n",
      "Current Epoch :  80  Current Accuracy :  0.988558374811563  current loss : 0.01144162518843705  Current Learning Rate:  0.1\n",
      "\n",
      "Current Epoch :  81  Current Accuracy :  0.9885604715162821  current loss : 0.011439528483717851  Current Learning Rate:  0.1\n",
      "\n",
      "Current Epoch :  82  Current Accuracy :  0.9885625675304152  current loss : 0.011437432469584893  Current Learning Rate:  0.1\n",
      "\n",
      "Current Epoch :  83  Current Accuracy :  0.9885646628542836  current loss : 0.011435337145716457  Current Learning Rate:  0.1\n",
      "\n",
      "Current Epoch :  84  Current Accuracy :  0.988566757488209  current loss : 0.011433242511791028  Current Learning Rate:  0.1\n",
      "\n",
      "Current Epoch :  85  Current Accuracy :  0.9885688514325127  current loss : 0.011431148567487278  Current Learning Rate:  0.1\n",
      "\n",
      "Current Epoch :  86  Current Accuracy :  0.9885709446875159  current loss : 0.011429055312484063  Current Learning Rate:  0.1\n",
      "\n",
      "Current Epoch :  87  Current Accuracy :  0.9885730372535395  current loss : 0.011426962746460458  Current Learning Rate:  0.1\n",
      "\n",
      "Current Epoch :  88  Current Accuracy :  0.9885751291309043  current loss : 0.011424870869095695  Current Learning Rate:  0.1\n",
      "\n",
      "Current Epoch :  89  Current Accuracy :  0.9885772203199308  current loss : 0.011422779680069214  Current Learning Rate:  0.1\n",
      "\n",
      "Current Epoch :  90  Current Accuracy :  0.9885793108209393  current loss : 0.011420689179060646  Current Learning Rate:  0.1\n",
      "\n",
      "Current Epoch :  91  Current Accuracy :  0.9885814006342502  current loss : 0.011418599365749824  Current Learning Rate:  0.1\n",
      "\n",
      "Current Epoch :  92  Current Accuracy :  0.9885834897601833  current loss : 0.011416510239816768  Current Learning Rate:  0.1\n",
      "\n",
      "Current Epoch :  93  Current Accuracy :  0.9885855781990583  current loss : 0.011414421800941668  Current Learning Rate:  0.1\n",
      "\n",
      "Current Epoch :  94  Current Accuracy :  0.9885876659511951  current loss : 0.011412334048804935  Current Learning Rate:  0.1\n",
      "\n",
      "Current Epoch :  95  Current Accuracy :  0.9885897530169129  current loss : 0.011410246983087149  Current Learning Rate:  0.1\n",
      "\n",
      "Current Epoch :  96  Current Accuracy :  0.988591839396531  current loss : 0.011408160603469109  Current Learning Rate:  0.1\n",
      "\n",
      "Current Epoch :  97  Current Accuracy :  0.9885939250903683  current loss : 0.011406074909631768  Current Learning Rate:  0.1\n",
      "\n",
      "Current Epoch :  98  Current Accuracy :  0.9885960100987438  current loss : 0.011403989901256286  Current Learning Rate:  0.1\n",
      "\n",
      "Current Epoch :  99  Current Accuracy :  0.988598094421976  current loss : 0.011401905578024021  Current Learning Rate:  0.1\n"
     ]
    }
   ],
   "source": [
    "# Here is the set of training opertions with weight updation included...\n",
    "\n",
    "for iter in range(numer_of_epoch):\n",
    "\n",
    "    # 1. Make the Dot Product operation\n",
    "    layer_1 = x.dot(w1)\n",
    "    layer_1_act = sigmoid(layer_1)\n",
    "\n",
    "    # loss function - Mean Square Error(MSE) 0.5\n",
    "    loss = np.square(layer_1_act - y) / (len(layer_1_act)  * 2)\n",
    "\n",
    "    print (\"\\nCurrent Epoch : \",iter ,\" Current Accuracy : \",1- loss.sum(),\" current loss :\", loss.sum(),\" Current Learning Rate: \",learning_rate)\n",
    "\n",
    "    # Stocastic Gradient Descent(SGD) - BATCH\n",
    "    # this is the training algorithm here...\n",
    "    grad_1_part_1 = (layer_1_act - y) / len(layer_1_act)\n",
    "    grad_1_part_2 = d_sigmoid(layer_1)\n",
    "    grad_1_part_3 = x\n",
    "    grad_1 = grad_1_part_3.T.dot(grad_1_part_1 * grad_1_part_2)\n",
    "\n",
    "    # Weight Update\n",
    "    w1 -=  learning_rate * grad_1\n",
    "    \n",
    "    # To keep things easy to understand, we have hardcoaded\n",
    "    # the steps where the learning rate is changed...\n",
    "    # Traditionally it is to be updated automatically if no change is detected in\n",
    "    # Accuracy and loss over a number of epoch...\n",
    "    if iter == 50 :\n",
    "        learning_rate = 1\n",
    "        \n",
    "    if iter == 70 :\n",
    "        learning_rate = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we declare a single layer NN...\n",
    "\n",
    "layer_1 = x.dot(w1)\n",
    "layer_1_act = sigmoid(layer_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Final :  [0.00906602 0.16232736 0.1620407  0.80376285]\n",
      "Final Round:  [0. 0. 0. 1.]\n",
      "Ground Truth :  [0 0 0 1]\n",
      "W1 :  [ 3.05099269  3.05310233 -4.69411465]\n"
     ]
    }
   ],
   "source": [
    "# what we get this time?\n",
    "print (\"\\n\\nFinal : \" ,layer_1_act[:,-1])\n",
    "print (\"Final Round: \" ,np.round(layer_1_act[:,-1]))\n",
    "print (\"Ground Truth : \",y[:,-1])\n",
    "print (\"W1 : \",w1[:,-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Does updating weights and learning rates made a difference?\n",
    "### Let's compare the outputs of both cycles..."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Previous Training method returned the following outcome...\n",
    "\n",
    "Final :  [0.09554338 0.24983353 0.3376184  0.61640766]\n",
    "\n",
    "\n",
    "The new approach returned this:\n",
    "\n",
    "Final :  [0.00828867 0.15834097 0.15811772 0.80870512]\n",
    "\n",
    "We had to reach:\n",
    "\n",
    "Final :  [0 0 0 1]\n",
    "\n",
    "\n",
    "Clearly the second method returned more accurate results..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------------That's it for this cycle-----------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## But what if we extend the same approach to 2 layers...\n",
    "## Only one way to find out..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a new Random seed...\n",
    "np.random.seed(1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets use 'Tan-hyperbolic' activation function this time...\n",
    "\n",
    "def tanh(x):\n",
    "    return np.tanh(x)\n",
    "\n",
    "def d_tanh(x):\n",
    "    return 1 - tanh(x) ** 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w1 =  [[ 0.47143516 -1.19097569  1.43270697 -0.3126519  -0.72058873]\n",
      " [ 0.88716294  0.85958841 -0.6365235   0.01569637 -2.24268495]\n",
      " [ 1.15003572  0.99194602  0.95332413 -2.02125482 -0.33407737]]\n",
      "\n",
      "w2 =  [[ 0.00211836]\n",
      " [ 0.40545341]\n",
      " [ 0.28909194]\n",
      " [ 1.32115819]\n",
      " [-1.54690555]]\n"
     ]
    }
   ],
   "source": [
    "# Initialize the weights for layers 'l1' & 'l2'...\n",
    "\n",
    "w1 = np.random.randn(3,5)\n",
    "w2 = np.random.randn(5,1)\n",
    "\n",
    "print('w1 = ', w1)\n",
    "print('\\nw2 = ', w2)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "weights are initialized according to the input data which is same throughout the notebook, i.e.\n",
    "\n",
    "x = np.array([\n",
    "    [0,0,1],\n",
    "    [0,1,1],\n",
    "    [1,0,1],\n",
    "    [1,1,1]\n",
    "])\n",
    "\n",
    "y = np.array([\n",
    "    [0],\n",
    "    [0],\n",
    "    [0],\n",
    "    [1]\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Declare Hyper Parameters...\n",
    "\n",
    "# Set the number of epoches\n",
    "numer_of_epoch = 250\n",
    "\n",
    "# Set the learning rate\n",
    "learning_rate = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training the network...\n",
    "\n",
    "for iter in range(numer_of_epoch):\n",
    "\n",
    "    layer_1 = x.dot(w1)\n",
    "    layer_1_act = tanh(layer_1)\n",
    "\n",
    "    layer_2 = layer_1_act.dot(w2)\n",
    "    layer_2_act = tanh(layer_2)\n",
    "\n",
    "    cost = np.square(layer_2_act - y).sum() / len(x)\n",
    "\n",
    "    grad_2_part_1 = (2/len(x)) * (layer_2_act - y)\n",
    "    grad_2_part_2 = d_tanh(layer_2)\n",
    "    grad_2_part_3 = layer_1_act\n",
    "    grad_2 =   grad_2_part_3.T.dot(grad_2_part_1 * grad_2_part_2) \n",
    "\n",
    "    grad_1_part_1 = (grad_2_part_1 * grad_2_part_2).dot(w2.T)\n",
    "    grad_1_part_2 = d_tanh(layer_1)\n",
    "    grad_1_part_3 = x\n",
    "    grad_1 =   grad_1_part_3.T.dot(grad_1_part_1 * grad_1_part_2)\n",
    "    \n",
    "    # Update the weights...\n",
    "\n",
    "    w1 -= 0.01*grad_1\n",
    "    w2 -= 0.1*grad_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.03024685]\n",
      " [0.05881942]\n",
      " [0.08153515]\n",
      " [0.71023473]]\n"
     ]
    }
   ],
   "source": [
    "# This is the Architecture...\n",
    "\n",
    "layer_2 = layer_1_act.dot(w2)\n",
    "layer_2_act = tanh(layer_2)\n",
    "print(layer_2_act)\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Compared to the previous method, it worked pretty well.\n",
    "\n",
    "We can increase the accuracy with more epoch(not always nacessary) and better training algorithm..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
